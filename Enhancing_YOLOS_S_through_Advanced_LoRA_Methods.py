# -*- coding: utf-8 -*-
"""Enhancing YOLOS-S through Advanced LoRA Methods

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fA7puuxSJUs2AKdflqyJpBS_xDLOS66_

# **Enhancing Object Detection in YOLOS-Small through Advanced LoRA Methods**

## **1. Setting Up the Environment**
"""

# Install necessary libraries
!pip install torch torchvision transformers datasets huggingface_hub peft loralib
!pip install -q bitsandbytes accelerate loralib
!pip install torchmetrics torchinfo
!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
!pip install --upgrade datasets
!pip install --upgrade git+https://github.com/huggingface/peft.git
!pip install tqdm
!pip install transformers tensorboard datasets evaluate torch torchvision -q

# Import necessary libraries
import os
import random
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image, ImageDraw, ImageFont
from torch.utils.data import DataLoader, Dataset
from torchinfo import summary
from torchmetrics.detection.mean_ap import MeanAveragePrecision
from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau
import torch.nn.functional as F
from datasets import load_dataset, get_dataset_split_names
from transformers import (
    YolosForObjectDetection,
    AutoImageProcessor
)
from peft import get_peft_model, LoraConfig, AdaLoraConfig, LoHaConfig, LoKrConfig, PeftModel
from collections import Counter, defaultdict
from IPython.display import display

# Set random seed for reproducibility
SEED_GLOBAL = 42
random.seed(SEED_GLOBAL)
np.random.seed(SEED_GLOBAL)
torch.manual_seed(SEED_GLOBAL)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED_GLOBAL)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Set device to GPU if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

"""## **2. Data Preparation**

### Load COCO Dataset
"""

# Get COCO datasets split
split_names = get_dataset_split_names("detection-datasets/coco")
print(f"coco split dataset: {split_names}")

# Load COCO Validation dataset - 4,952 images
dataset = load_dataset("detection-datasets/coco", streaming=True)
val_dataset = dataset["val"]

# Map category indices to labels
coco_metadata = load_dataset("detection-datasets/coco", split="train")  # Load metadata
coco_categories = coco_metadata.features["objects"].feature["category"].names
coco_categories = {index: x for index, x in enumerate(coco_categories, start=0)}
coco_label2id = {v: k for k, v in coco_categories.items()}
print(f"COCO {len(coco_categories)} categories mapping (label2id): {coco_categories}")

"""### Load YOLOS-Small Pre-trained Model"""

model_name = "hustvl/yolos-small"
original_model = YolosForObjectDetection.from_pretrained(model_name).to(device)
processor = AutoImageProcessor.from_pretrained(model_name)

# Load YOLOS categories
yolos_categories = original_model.config.id2label
print(f"YOLOS {original_model.config.num_labels} categories mapping (label2id): {yolos_categories}")

"""## **3. Data Understanding**

### YOLOS-Small Architechture
"""

summary(original_model)

"""### Processor Configuration"""

processor

"""### Data Structure"""

# Checking dataset structure using first image
print("\nValidation dataset structure:")
val_example = next(iter(val_dataset))
val_image = val_example["image"]  # Already a PIL.Image object
print(f"Original Validation Image Size: {val_image.size}")
print("Example keys:", val_example.keys())
print("Type of image:", type(val_example["image"]))

# Checking the structure of objects
if "objects" in val_example:
    validation_objects = val_example["objects"]
    print(f"Validation objects structure: {type(validation_objects)}")
    if isinstance(validation_objects, dict):
        print(f"Keys in objects: {list(validation_objects.keys())}")
        print(f"bbox_id:{validation_objects['bbox_id']}")
        print(f"category:{validation_objects['category']}")
        print(f"bbox:{validation_objects['bbox']}")
        print(f"area:{validation_objects['area']}")
    elif isinstance(validation_objects, list):
        print(f"First object: {validation_objects[0]}")

"""### Categories Comparison between YOLOS and COCO"""

# Null YOLOS categories
nulls = "Null YOLOS Categories: "
for k, v in yolos_categories.items():
    if v == "N/A":
        nulls += f"{k}, "
nulls = nulls.rstrip(", ")
print(nulls)

# Compare YOLOS and COCO categories
yolos_to_coco_mapping = {}
mismatched_categories = []
for yolos_id, yolos_label in yolos_categories.items():
    coco_match = None
    for coco_id, coco_label in coco_categories.items():
        if yolos_label.lower() == coco_label.lower():  # Compare by name
            coco_match = coco_id
            break
    if coco_match is not None:
        yolos_to_coco_mapping[yolos_id] = coco_match
    else:
        mismatched_categories.append((yolos_id, yolos_label))

# Print matched categories
print("\nMatched categories:")
for yolos_id, coco_id in yolos_to_coco_mapping.items():
    print(f"YOLOS ID {yolos_id}: {yolos_categories[yolos_id]} -> COCO ID {coco_id}: {coco_categories[coco_id]}")

# Print mismatched categories
print("\nMismatched categories:")
for yolos_id, yolos_label in mismatched_categories:
    print(f"YOLOS ID {yolos_id}: {yolos_label} (No match in COCO)")

# Summary of results
print("\nComparison summary:")
print(f"Total YOLOS categories: {len(yolos_categories)}")
print(f"Matched categories: {len(yolos_to_coco_mapping)}")
print(f"Mismatched categories: {len(mismatched_categories)}")
# Check if all categories in COCO can be detected by the model
coco_categories_set = set(coco_categories.values())
imagenet_categories_set = set(yolos_categories.values())
missing_categories = coco_categories_set - imagenet_categories_set
if len(missing_categories) == 0:
    print("YOLOS model can detect all COCO categories.")
else:
    print(f"YOLOS model cannot detect {len(missing_categories)} COCO categories.")

# Create a mapping between YOLOS category IDs and COCO category IDs for evaluation
yolos_to_coco_mapping = {yolos_id: yolos_to_coco_mapping.get(yolos_id, -1) for yolos_id in yolos_categories.keys()}
print(f"YOLOS to COCO categories mapping: {yolos_to_coco_mapping}")
# Create a reverse mapping to go from COCO category IDs back to YOLOS category IDs
coco_to_yolos_mapping = {coco_id: yolos_id for yolos_id, coco_id in yolos_to_coco_mapping.items() if coco_id != -1}
print(f"COCO to YOLOS categories mapping: {coco_to_yolos_mapping}")

"""### Sample Visualization"""

def visualize_sample(image, objects, title="Sample Visualization", font_size=16):
    if isinstance(image, np.ndarray) and image.ndim == 2: # Handle 2D tensor images
        image = Image.fromarray(np.uint8(image)).convert("RGB")
    elif isinstance(image, Image.Image): # Handle grayscale images
        image = image.convert("RGB")
    else:
        print(f"Unsupported image format. Skipping.")
        return

    draw = ImageDraw.Draw(image)
    try:
        font = ImageFont.truetype("arial.ttf", font_size)
    except:
        font = ImageFont.load_default()
    if isinstance(objects, dict): # ensure the required keys
        bbox_list = objects.get("bbox", [])
        category_list = objects.get("category", [])
        if len(bbox_list) != len(category_list):
            print("Mismatch between bbox and category lengths.")
            return

    # Draw Bounding Boxes
    for category_id, box in zip(category_list, bbox_list):
        label = coco_categories[category_id]
        draw.rectangle(box, outline="red", width=2)
        draw.text((box[0], box[1] - font_size), label, fill="red", font=font)
    plt.figure(figsize=(12, 8))
    plt.title(title)
    plt.imshow(image)
    plt.axis("off")
    plt.show()

# Visualize first sample
val_image = val_example["image"]
val_image_annotations = val_example["objects"]
print(f"Validation Sample Image Size: {val_image.size}")
print(f"Validation Sample Annotations: {val_image_annotations}")
visualize_sample(val_image, val_image_annotations, title=f"Ground Truth of Image {val_example['image_id']}")

"""## **4. Split COCO Validation dataset**

### Train and Test Split of COCO Validation dataset
"""

# shaffle dataset for splitting datasets
val_dataset = val_dataset.shuffle(seed=SEED_GLOBAL).shuffle(seed=SEED_GLOBAL).shuffle(seed=SEED_GLOBAL)

# Choose Train and Test smaples from COCO Validation dataset
train_full_dataset = val_dataset.skip(900).take(2100)
test_dataset = val_dataset.take(900)
train_full_count = sum(1 for _ in train_full_dataset)
test_count = sum(1 for _ in test_dataset)
print(f"Train (and validation) dataset size: {train_full_count}")
print(f"Test dataset size: {test_count}")

"""### Train and Test datasets structure"""

# Train
print("\nTrain dataset structure:")
train_example = next(iter(train_full_dataset.shuffle(seed=SEED_GLOBAL)))
train_image = train_example["image"]
print(f"Original Train Image Size: {train_image.size}")
print("Example keys:", train_example.keys())
print("Type of image:", type(train_example["image"]))
if "objects" in train_example:
    train_objects = train_example["objects"]
    print(f"Train objects structure: {type(train_objects)}")
    print(f"Keys in objects: {list(train_objects.keys())}")
    print(f"bbox_id:{train_objects['bbox_id']}")
    print(f"category:{train_objects['category']}")
    print(f"bbox:{train_objects['bbox']}")
    print(f"area:{train_objects['area']}")

# Test
print("\nTest dataset structure:")
test_dataset = test_dataset
test_example = next(iter(test_dataset))
test_image = test_example["image"]
print(f"Original Validation Image Size: {test_image.size}")
print("Example keys:", test_example.keys())
print("Type of image:", type(test_example["image"]))
if "objects" in test_example:
    test_objects = test_example["objects"]
    print(f"Test objects structure: {type(test_objects)}")
    print(f"Keys in objects: {list(test_objects.keys())}")
    print(f"bbox_id:{test_objects['bbox_id']}")
    print(f"category:{test_objects['category']}")
    print(f"bbox:{test_objects['bbox']}")
    print(f"area:{test_objects['area']}")

"""### Visualize Train and Test samples"""

# Train
train_image = train_example["image"]
train_image_annotations = train_example["objects"]
print(f"Train Sample Image Size: {train_image.size}")
print(f"Train Sample Annotations: {train_image_annotations}")
visualize_sample(train_image.copy(), train_image_annotations, title=f"Train Image {train_example['image_id']}")

# Test
test_image = test_example["image"]
test_image_annotations = test_example["objects"]
print(f"Test Sample Image Size: {test_image.size}")
print(f"Test Sample Annotations: {test_image_annotations}")
visualize_sample(test_image.copy(), test_image_annotations, title=f"Test Image {test_example['image_id']}")

"""### Split Train full dataset into Train and Validation datasets"""

train_dataset = train_full_dataset.skip(630) # 70% train
train_val_dataset = train_full_dataset.take(630) # 30% validation
train_count = sum(1 for _ in train_dataset)
train_val_count = sum(1 for _ in train_val_dataset)
print(f"Train dataset size: {train_count}  ({int(train_count/train_full_count*100)}% train samples)")
print(f"Validation dataset size: {train_val_count}  ({int(train_val_count/train_full_count*100)}% train samples)")

"""### Ensuring Representation of All Categories"""

def count_categories(dataset):
    category_counts = defaultdict(int)
    total_images = 0
    for sample in dataset:
        total_images += 1
        categories = sample["objects"]["category"]
        # Ensure each image is counted only once per category
        unique_categories = set(categories)
        for cat in unique_categories:
            category_counts[cat] += 1
    return category_counts, total_images

# Count for train, validation and test sets
train_category_counts, train_total = count_categories(train_dataset)
train_val_category_counts, train_val_total = count_categories(train_val_dataset)
test_category_counts, test_total = count_categories(test_dataset)

# Expected categories from COCO (0-79)
all_categories = set(range(80))

# Find missing categories
missing_train_categories = all_categories - set(train_category_counts.keys())
missing_train_val_categories = all_categories - set(train_val_category_counts.keys())
missing_test_categories = all_categories - set(test_category_counts.keys())

# Category Counts
# Train
print(f"Total Train Images: {train_total}")
print("Train Set Category Distribution:")
for category in sorted(all_categories):
    count = train_category_counts.get(category, 0)
    category_name = coco_categories[category]  # Get category name
    print(f"{category_name} (Category {category}): {count} images")
# Validation
print(f"\nTotal Train-Validation Images: {train_val_total}")
print("Train-Validation Set Category Distribution:")
for category in sorted(all_categories):
    count = train_val_category_counts.get(category, 0)
    category_name = coco_categories[category]  # Get category name
    print(f"{category_name} (Category {category}): {count} images")
# Test
print(f"\nTotal Test Images: {test_total}")
print("Test Set Category Distribution:")
for category in sorted(all_categories):
    count = test_category_counts.get(category, 0)
    category_name = coco_categories[category]  # Get category name
    print(f"{category_name} (Category {category}): {count} images")

# Missing Categories
# Train
if missing_train_categories:
    missing_names = [f"{coco_categories[cat]} ({cat})" for cat in sorted(missing_train_categories)]
    print(f"\nMissing categories in train set: {missing_names}")
else:
    print("\nAll categories are represented in the TRAIN set!")
# Validation
if missing_train_val_categories:
    missing_names = [f"{coco_categories[cat]} ({cat})" for cat in sorted(missing_train_val_categories)]
    print(f"\nMissing categories in validation set: {missing_names}")
else:
    print("\nAll categories are represented in the VALIDATION set!")
# Test
if missing_test_categories:
    missing_names = [f"{coco_categories[cat]} ({cat})" for cat in sorted(missing_test_categories)]
    print(f"\nMissing categories in test set: {missing_names}")
else:
    print("\nAll categories are represented in the TEST set!")

# Shuffle all datasets
train_dataset = train_dataset.shuffle(seed=SEED_GLOBAL)
train_val_dataset = train_val_dataset.shuffle(seed=SEED_GLOBAL)
test_dataset = test_dataset.shuffle(seed=SEED_GLOBAL)

"""## **5. mAP Evaluation**

### Visualization Function
"""

def visualize_bounding_boxes(image, results, id2label, title):
    draw = ImageDraw.Draw(image)
    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        box = [round(i, 2) for i in box.tolist()]
        x, y, x2, y2 = tuple(box)
        draw.rectangle((x, y, x2, y2), outline="red", width=2)
        draw.text((x, y), id2label[label.item()], fill="white")
    plt.figure(figsize=(12, 8))
    plt.title(title)
    plt.imshow(image)
    plt.axis("off")
    plt.show()

"""### Evaluate Model using mAP Function"""

def evaluate_model(model, processor, data, id2label, confidence_threshold=0.5, iou_threshold=0.5, add_class_metrices=False):
    model.eval()

    # mAP for COCO benchmarks - define IoU thresholds ranging from 0.5 to 0.95
    iou_thresholds_for_mAP = torch.arange(0.5, 1.0, 0.05).tolist()

    if add_class_metrices:
        metric = MeanAveragePrecision(iou_thresholds=iou_thresholds_for_mAP, class_metrics=True)
    else:
        metric = MeanAveragePrecision(iou_thresholds=iou_thresholds_for_mAP)

    results = []
    references = []
    first_image_flag = True

    for sample in tqdm(data):
        idx = sample["image_id"]
        image = sample["image"]
        if image.mode != "RGB": # Convert to RGB (3 channels)
            image = image.convert("RGB")

        inputs = processor(images=image, return_tensors="pt").to(device)

        outputs = model(**inputs) # bboxes and logits
        target_sizes = torch.tensor([image.size[::-1]], device=device)
        processed_outputs = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=confidence_threshold)[0]

        if first_image_flag:
            for score, label, box in zip(processed_outputs["scores"], processed_outputs["labels"], processed_outputs["boxes"]):
                box = [round(i, 2) for i in box.tolist()]
                print(
                    f"Detected {model.config.id2label[label.item()]} with confidence "
                    f"{round(score.item(), 3)} at location {box}"
                )

        # Extract predictions
        pred_boxes = processed_outputs["boxes"].detach().cpu()
        pred_scores = processed_outputs["scores"].detach().cpu()

        if first_image_flag:
            visualize_bounding_boxes(image.copy(), processed_outputs, id2label, f"YOLOS Predictions for Image {idx}")
            first_image_flag = False

        # Exclude YOLOS null categories
        null_labels_yolos_indexes = {0, 12, 26, 29, 30, 45, 66, 68, 69, 71, 83}
        valid_indices = [
            i for i, label in enumerate(processed_outputs["labels"].detach().cpu())
            if label.item() not in null_labels_yolos_indexes
        ]
        pred_boxes = pred_boxes[valid_indices]
        pred_scores = pred_scores[valid_indices]
        filtered_labels = processed_outputs["labels"].detach().cpu()[valid_indices]

        # Map YOLOS valid labels to COCO
        pred_labels = torch.tensor([yolos_to_coco_mapping[label.item()] for label in filtered_labels])

        # Predictions
        results.append({
            "image_id": sample["image_id"],
            "boxes": pred_boxes,
            "scores": pred_scores,
            "labels": pred_labels
        })

        # Ground Truth
        gt_boxes = torch.tensor(sample["objects"]["bbox"], dtype=torch.float32)
        gt_labels = torch.tensor(sample["objects"]["category"], dtype=torch.int64)
        references.append({
            "image_id": sample["image_id"],
            "annotations": {
                "boxes": gt_boxes,
                "labels": gt_labels
            }
        })

    # Compute metrices
    for result, reference in zip(results, references):
        metric.update(
            preds=[{
                "boxes": result["boxes"],
                "scores": result["scores"],
                "labels": result["labels"]
            }],
            target=[{
                "boxes": reference["annotations"]["boxes"],
                "labels": reference["annotations"]["labels"]
            }]
        )
    metrics = metric.compute()

    # Results
    print("mAP@[0.5:0.95]:", metrics["map"])
    print("Detailed mAP results:", metrics)
    return metrics

"""### Calculate Baseline score"""

baseline_results = evaluate_model(original_model,processor, test_dataset, yolos_categories)
print("Baseline Performance (mAP):", baseline_results["map"])

baseline_results_dict = {
    key: value.item() if isinstance(value, torch.Tensor) else value
    for key, value in baseline_results.items()
    if key != "classes"
}
baseline_results_df = pd.DataFrame([baseline_results_dict])
print("Baseline Performance (mAP):")
print(baseline_results_df)
display(baseline_results_df)

"""## **6. Implementing LoRA Variants**

### Inspecting Potential Layers
"""

model = YolosForObjectDetection.from_pretrained(model_name).to(device)
for name, param in model.named_parameters():
    print(name)

"""### Define Target Modules"""

target_modules = [
    f"vit.encoder.layer.{i}.attention.attention.query" for i in range(12)
] + [
    f"vit.encoder.layer.{i}.attention.attention.value" for i in range(12)
]

"""### Functions for Model Training"""

class CustomDataset(Dataset):
    def __init__(self, data_list):
        self.data = data_list

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# Train
train_dataset_list = list(train_dataset)
train_dataset_fixed = CustomDataset(train_dataset_list)

# Val
train_val_dataset_list = list(train_val_dataset)
train_val_dataset_fixed = CustomDataset(train_val_dataset_list)

def collate_fn(batch):
    images = [sample["image"].convert("RGB") if sample["image"].mode != "RGB" else sample["image"] for sample in batch]

    annotations = []
    for sample in batch:
        image_id = sample["image_id"]

        if "objects" in sample and "category" in sample["objects"] and "bbox" in sample["objects"]:
            formatted_annotations = [
                {
                    "bbox": bbox,
                    "category_id": coco_to_yolos_mapping.get(cat_id, None),  # Map COCO to YOLOS categories
                    "area": area
                }
                for bbox, cat_id, area in zip(sample["objects"]["bbox"], sample["objects"]["category"], sample["objects"]["area"])
                if coco_to_yolos_mapping.get(cat_id, None) is not None  # Exclude invalid categories
            ]
            annotations.append({
                "image_id": image_id,
                "annotations": formatted_annotations
            })
        else:
            annotations.append({"image_id": image_id, "annotations": []})

    # Use processor to handle resizing, padding, and bounding box adaptation
    # from COCO [x_min, y_min, width, hight] to YOLOS [x_center, y_center, width, height]
    inputs = processor(images=images, annotations=annotations, return_tensors="pt")

    # Extract processed images and labels
    processed_images = inputs["pixel_values"]
    labels = inputs["labels"]

    return {"pixel_values": processed_images, "labels": labels}


train_dataloader = DataLoader(train_dataset_fixed, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(train_val_dataset_fixed, batch_size=4, shuffle=False, collate_fn=collate_fn)

# Early Stopping - stop training if mAP does not improve
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_map = -float("inf")
        self.counter = 0

    def __call__(self, current_map):
        if current_map > self.best_map + self.min_delta:
            self.best_map = current_map
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                print("Early stopping triggered - Training Stopped!")
                return True  # sign to stop training
        return False

def focal_loss(logits, targets, alpha=0.25, gamma=2.0, null_labels={0, 12, 26, 29, 30, 45, 66, 68, 69, 71, 83}):
    batch_size, num_queries, num_classes = logits.shape
    device = logits.device
    null_labels_tensor = torch.tensor(list(null_labels), device=device)
    log_probs = F.log_softmax(logits, dim=-1)
    target_tensor = torch.full((batch_size, num_queries), fill_value=-1, dtype=torch.long, device=device) # -1 means ignored label

    for i in range(batch_size):
        if "class_labels" not in targets[i]:
            continue
        labels = targets[i]["class_labels"].to(device)
        valid_labels = labels[~torch.isin(labels, null_labels_tensor)] # Filter out null labels
        num_valid_labels = min(valid_labels.numel(), num_queries)
        target_tensor[i, :num_valid_labels] = valid_labels[:num_valid_labels]

    # Apply mask to ignore invalid labels
    valid_mask = target_tensor != -1
    gathered_log_probs = log_probs[torch.arange(batch_size).unsqueeze(1), torch.arange(num_queries).unsqueeze(0), target_tensor]
    gathered_log_probs = gathered_log_probs * valid_mask

    # Compute focal loss
    pt = gathered_log_probs.exp()
    focal_weight = alpha * (1 - pt) ** gamma
    loss = -focal_weight * gathered_log_probs
    loss = loss.sum() / valid_mask.sum()
    return loss

"""### Training Function"""

def train_model(model, processor, train_dataloader, val_dataloader, optimizer, scheduler, early_stopping, device, output_model_path, epochs=5, accumulation_steps=4, is_plateau_scheduler=False):
    model.to(device)
    model.train()

    epochs_list = []
    train_total_losses = []
    val_total_losses = []
    train_focal_losses = []
    train_bbox_losses = []
    train_giou_losses = []
    mAP_scores = []

    best_mAP_score = 0

    for epoch in range(epochs):
        total_loss = 0
        skipped_batches = 0
        epoch_focal_loss = 0
        epoch_bbox_loss = 0
        epoch_giou_loss = 0

        model.train()

        optimizer.zero_grad()

        for batch_idx, batch in enumerate(tqdm(train_dataloader)):
            batch_inputs = batch["pixel_values"].to(device)
            batch_labels = [{k: v.to(device) for k, v in label.items()} for label in batch["labels"]]

            outputs = model(**{"pixel_values": batch_inputs}, labels=batch_labels)

            if outputs.loss is None:
                print(f"No loss for batch {batch_idx}, skipping")
                skipped_batches += 1
                continue

            # Loss components (with focal loss instead of the default-CE)
            loss_focal = focal_loss(outputs.logits, batch_labels)
            loss_bbox = outputs.loss_dict["loss_bbox"]
            loss_giou = outputs.loss_dict["loss_giou"]
            total_loss_batch = 1.5 * loss_focal + 1.0 * loss_bbox + 2.0 * loss_giou

            # Gradient accumulation
            scaled_loss = total_loss_batch / accumulation_steps
            scaled_loss.backward()

            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1 == len(train_dataloader)):
                optimizer.step()
                optimizer.zero_grad()

            total_loss += total_loss_batch.item()
            epoch_focal_loss += loss_focal.item()
            epoch_bbox_loss += loss_bbox.item()
            epoch_giou_loss += loss_giou.item()

            if batch_idx % 10 == 0:
                print(f"Epoch {epoch + 1}, Batch {batch_idx}: Loss = {scaled_loss.item():.4f}")

            # Clear memory after processing batch
            del batch, batch_inputs, batch_labels, outputs
            torch.cuda.empty_cache()

        # Compute average training losses
        num_batches = len(train_dataloader) - skipped_batches
        avg_train_loss = total_loss / num_batches if num_batches > 0 else 0
        avg_focal_loss = epoch_focal_loss / num_batches if num_batches > 0 else 0
        avg_bbox_loss = epoch_bbox_loss / num_batches if num_batches > 0 else 0
        avg_giou_loss = epoch_giou_loss / num_batches if num_batches > 0 else 0

        print(f"Epoch {epoch + 1} Complete. Average train Loss: {avg_train_loss:.4f}, Skipped Batches: {skipped_batches}")

        # Store losses
        epochs_list.append(epoch + 1)
        train_total_losses.append(avg_train_loss)
        train_focal_losses.append(avg_focal_loss)
        train_bbox_losses.append(avg_bbox_loss)
        train_giou_losses.append(avg_giou_loss)

        # Evaluation on Validation Set
        model.eval()
        total_val_loss = 0
        skipped_val_batches = 0
        with torch.no_grad():
            for val_batch in tqdm(val_dataloader, desc="Validating"):
                val_batch_inputs = val_batch["pixel_values"].to(device)
                val_batch_labels = [{k: v.to(device) for k, v in label.items()} for label in val_batch["labels"]]

                val_outputs = model(**{"pixel_values": val_batch_inputs}, labels=val_batch_labels)

                if val_outputs.loss is None:
                    print(f"No loss for a validation batch, skipping")
                    skipped_val_batches += 1
                    continue

                val_loss_focal = focal_loss(val_outputs.logits, val_batch_labels)
                val_loss_bbox = val_outputs.loss_dict["loss_bbox"]
                val_loss_giou = val_outputs.loss_dict["loss_giou"]
                val_loss_batch = 1.5 * val_loss_focal + 1.0 * val_loss_bbox + 2.0 * val_loss_giou
                total_val_loss += val_loss_batch


        num_val_batches = len(val_dataloader) - skipped_val_batches
        avg_val_loss = total_val_loss / num_val_batches if num_val_batches > 0 else 0
        val_total_losses.append(avg_val_loss)
        print(f"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}")

        # Evaluate mAP on Validation Set
        model_val_results = evaluate_model(model, processor, train_val_dataset_fixed, yolos_categories)
        epoch_val_mAP_score = model_val_results['map'].item()
        print(f"Epoch {epoch+1} - Validation mAP Score: {epoch_val_mAP_score:.4f}")
        mAP_scores.append(epoch_val_mAP_score)

        model_val_results_dict = {key: value.item() if isinstance(value, torch.Tensor) else value
                                  for key, value in model_val_results.items()
                                  if key != "classes"}
        model_val_results_df = pd.DataFrame([model_val_results_dict])
        display(model_val_results_df)

        # Save the best model based on mAP score
        if epoch_val_mAP_score > best_mAP_score:
            best_mAP_score = epoch_val_mAP_score
            torch.save(model, output_model_path)
            print(f"Best model saved with mAP {best_mAP_score:.4f} at epoch {epoch + 1}")

        # Update scheduler based on validation loss
        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(avg_val_loss)
        else:
            scheduler.step()

        # Check if needed to stop training
        if early_stopping(epoch_val_mAP_score):
            break

    return best_mAP_score, epochs_list, train_total_losses, val_total_losses, train_focal_losses, train_bbox_losses, train_giou_losses, mAP_scores

"""### Evaluate Model Functions"""

def convert_to_numpy(data):
    if isinstance(data, torch.Tensor):
        return data.cpu().detach().numpy()
    elif isinstance(data, list):
        return [convert_to_numpy(item) for item in data]
    return data

def plot_loss_graphs(epochs_list, train_total_losses, train_focal_losses, train_bbox_losses, train_giou_losses, val_total_losses):
    epochs_list = convert_to_numpy(epochs_list)
    train_total_losses = convert_to_numpy(train_total_losses)
    train_focal_losses = convert_to_numpy(train_focal_losses)
    train_bbox_losses = convert_to_numpy(train_bbox_losses)
    train_giou_losses = convert_to_numpy(train_giou_losses)
    val_total_losses = convert_to_numpy(val_total_losses)

    # Plot 1: Total Training Loss over Epochs
    plt.figure(figsize=(10, 5))
    plt.plot(epochs_list, train_total_losses, label="Train Loss", linestyle="-", marker="o", color="blue")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Train Loss Over Epochs")
    plt.legend()
    plt.grid()
    plt.show()

    # Plot 2: Training Loss Components Over Epochs
    plt.figure(figsize=(8, 6))
    plt.plot(epochs_list, train_focal_losses, label="Train Focal Loss", linestyle="--", marker="x", color="orange")
    plt.plot(epochs_list, train_bbox_losses, label="Train BBox Loss", linestyle="--", marker="s", color="green")
    plt.plot(epochs_list, train_giou_losses, label="Train GIoU Loss", linestyle="--", marker="d", color="red")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Train Loss Components Over Epochs")
    plt.legend()
    plt.grid()
    plt.show()

    # Plot 3: Training and Validation Loss Over Epochs
    plt.figure(figsize=(10, 5))
    plt.plot(epochs_list, train_total_losses, label="Train Loss", linestyle="-", marker="o", color="blue")
    plt.plot(epochs_list, val_total_losses, label="Validation Loss", linestyle="--", marker="x", color="green")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss Over Epochs")
    plt.legend()
    plt.grid()
    plt.show()

"""###  **(a) LoRA**

#### **LoraConfig**
"""

# Define model for LoRA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_for_LoRA = YolosForObjectDetection.from_pretrained(model_name).to(device)

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=8,
    lora_dropout=0.5,
    target_modules=target_modules
)

# Apply LoRA to the model
model_with_LoRA = get_peft_model(model_for_LoRA, lora_config)
model_with_LoRA.print_trainable_parameters()

# Inspect LoRA Layers
for name, param in model_with_LoRA.named_parameters():
    print(f"{name} - requires_grad: {param.requires_grad}")

for name, param in model_with_LoRA.named_parameters():
    if "lora" in name:
        print(name, param.shape)

# Mean value of LoRA layers before training
for name, param in model_with_LoRA.named_parameters():
    if "lora" in name:
        print(f"{name} mean before training: {param.data.mean().item():.6f}")

lora_total_parameters = [p for p in model_with_LoRA.parameters()]
print(f"Number of LoRA parameter tensors: {len(lora_total_parameters)}")

lora_parameters = [p for p in model_with_LoRA.parameters() if p.requires_grad]
print(f"Number of LoRA trainable parameter tensors (requires_grad=True): {len(lora_parameters)}")

"""#### **Train Model with LoRA**"""

# Train Setting
epochs = 10
optimizer = torch.optim.AdamW(lora_parameters, lr=5e-5, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-6)
early_stopping = EarlyStopping(patience=3)

# Train the LoRA model
best_mAP_score_LoRA, epochs_list_LoRA, train_total_losses_LoRA, val_total_losses_LoRA, train_focal_losses_LoRA, train_bbox_losses_LoRA, train_giou_losses_LoRA, mAP_scores_LoRA = train_model(
    model_with_LoRA, processor, train_dataloader, val_dataloader, optimizer, scheduler, early_stopping, device, "lora_model.pth", epochs
)

plot_loss_graphs(epochs_list_LoRA, train_total_losses_LoRA, train_focal_losses_LoRA, train_bbox_losses_LoRA, train_giou_losses_LoRA, val_total_losses_LoRA)

# Load the saved model with best mAP score
model_with_LoRA = torch.load("lora_model.pth", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print(model_with_LoRA.peft_config)

# Check which weights have been updated
for name, param in model_with_LoRA.named_parameters():
    if param.requires_grad:
        print(f"{name} Trainable (Updated)")
    else:
        print(f"{name} Frozen (Original)")

print(model_with_LoRA)

# Mean value of LoRA layers after training
for name, param in model_with_LoRA.named_parameters():
    if "lora" in name:
        print(f"{name} mean after training: {param.data.mean().item():.6f}")

"""#### **Evaluate Model with LoRA**"""

model_with_LoRA_results = evaluate_model(model_with_LoRA, processor, test_dataset, yolos_categories)
print("Model with LoRA Performance (mAP):", model_with_LoRA_results["map"])

model_with_LoRA_results_dict = {
    key: value.item() if isinstance(value, torch.Tensor) else value
    for key, value in model_with_LoRA_results.items()
    if key != "classes"
}
model_with_LoRA_results_df = pd.DataFrame([model_with_LoRA_results_dict])
print("Model with LoRA Performance (mAP):")
print(model_with_LoRA_results_df)
display(model_with_LoRA_results_df)

"""### **(b) AdaLoRA- Adaptive LoRA**

#### **AdaLoraConfig**
"""

# Define model for AdaLoRA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_for_AdaLoRA = YolosForObjectDetection.from_pretrained(model_name).to(device)

# Configure AdaLoRA
adalora_config = AdaLoraConfig(
    target_r=16,
    init_r=24,
    tinit=20,
    tfinal=40,
    total_step=100,
    deltaT=8,
    beta1=0.9,
    beta2=0.88,
    orth_reg_weight=0.01,
    target_modules=target_modules
)

# Apply AdaLoRA to the model
model_with_AdaLoRA = get_peft_model(model_for_AdaLoRA, adalora_config)
model_with_AdaLoRA.print_trainable_parameters()

# Inspect AdaLoRA Layers
for name, param in model_with_AdaLoRA.named_parameters():
    print(f"{name} - requires_grad: {param.requires_grad}")

for name, param in model_with_AdaLoRA.named_parameters():
    if "lora" in name:
        print(name, param.shape)

# Mean value of AdaLoRA layers before training
for name, param in model_with_AdaLoRA.named_parameters():
    if "lora" in name:
        print(f"{name} mean before training: {param.data.mean().item():.6f}")

adalora_total_parameters = [p for p in model_with_AdaLoRA.parameters()]
print(f"Number of AdaLoRA parameter tensors: {len(adalora_total_parameters)}")

adalora_parameters = [p for p in model_with_AdaLoRA.parameters() if p.requires_grad]
print(f"Number of AdaLoRA trainable parameter tensors (requires_grad=True): {len(adalora_parameters)}")

"""#### **Train Model with AdaLoRA**"""

# Train Setting
epochs = 10
optimizer = torch.optim.AdamW(adalora_parameters, lr=3e-5, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)
early_stopping = EarlyStopping(patience=3)

# Train the AdaLoRA model
best_mAP_scores_AdaLoRA, epochs_list_AdaLoRA, train_total_losses_AdaLoRA, val_total_losses_AdaLoRA, train_focal_losses_AdaLoRA, train_bbox_losses_AdaLoRA, train_giou_losses_AdaLoRA, mAP_scores_AdaLoRA = train_model(
    model_with_AdaLoRA, processor, train_dataloader, val_dataloader, optimizer, scheduler, early_stopping, device, "adalora_model.pth", epochs
)

plot_loss_graphs(epochs_list_AdaLoRA, train_total_losses_AdaLoRA, train_focal_losses_AdaLoRA, train_bbox_losses_AdaLoRA, train_giou_losses_AdaLoRA, val_total_losses_AdaLoRA)

# Load the saved model from last epoch
model_with_AdaLoRA = torch.load("adalora_model.pth", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print(model_with_AdaLoRA.peft_config)

# Check which weights have been updated
for name, param in model_with_AdaLoRA.named_parameters():
    if param.requires_grad:
        print(f"{name} Trainable (Updated)")
    else:
        print(f"{name} Frozen (Original)")

print(model_with_AdaLoRA)

# Mean value of AdaLoRA layers after training
for name, param in model_with_AdaLoRA.named_parameters():
    if "lora" in name:
        print(f"{name} mean after training: {param.data.mean().item():.6f}")

"""#### **Evaluate Model with AdaLoRA**"""

model_with_AdaLoRA_results = evaluate_model(model_with_AdaLoRA, processor, test_dataset, yolos_categories)
print("Model with AdaLoRA Performance (mAP):", model_with_AdaLoRA_results)

model_with_AdaLoRA_results_dict = {
    key: value.item() if isinstance(value, torch.Tensor) else value
    for key, value in model_with_AdaLoRA_results.items()
    if key != "classes"
}
model_with_AdaLoRA_results_df = pd.DataFrame([model_with_AdaLoRA_results_dict])
print("Model with AdaLoRA Performance (mAP):")
print(model_with_AdaLoRA_results_df)
display(model_with_AdaLoRA_results_df)

"""### **(c) LoHa- Low Rank Hadamard Product**

#### **LoHaConfig**
"""

# Define model for LoHa
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_for_LoHa = YolosForObjectDetection.from_pretrained(model_name).to(device)

# Configure LoHa
loha_config = LoHaConfig(
    r=8,
    alpha=12,
    rank_dropout=0.1,
    module_dropout=0.1,
    target_modules=target_modules
)

# Apply LoHa to the model
model_with_LoHa = get_peft_model(model_for_LoHa, loha_config)
model_with_LoHa.print_trainable_parameters()

# Inspect LoHa Layers
for name, param in model_with_LoHa.named_parameters():
    print(f"{name} - requires_grad: {param.requires_grad}")

for name, param in model_with_LoHa.named_parameters():
    if "hada" in name:
        print(name, param.shape)

# Mean value of LoHa layers before training
for name, param in model_with_LoHa.named_parameters():
    if "hada" in name:
        print(f"{name} mean before training: {param.data.mean().item():.6f}")

loha_total_parameters = [p for p in model_with_LoHa.parameters()]
print(f"Number of LoHA parameter tensors: {len(loha_total_parameters)}")

loha_parameters = [p for p in model_with_LoHa.parameters() if p.requires_grad]
print(f"Number of LoHA trainable parameter tensors (requires_grad=True): {len(loha_parameters)}")

"""#### **Train Model with LoHa**

"""

# Train Setting
epochs = 10
optimizer = torch.optim.AdamW(loha_parameters, lr=3e-5, weight_decay=0.01)
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2, eta_min=1e-6)
early_stopping = EarlyStopping(patience=3)

# Train the LoHa model
best_mAP_score_LoHa, epochs_list_LoHa, train_total_losses_LoHa, val_total_losses_LoHa, train_focal_losses_LoHa, train_bbox_losses_LoHa, train_giou_losses_LoHa, mAP_scores_LoHa = train_model(
    model_with_LoHa, processor, train_dataloader, val_dataloader, optimizer, scheduler, early_stopping, device, "loha_model.pth", epochs
)

plot_loss_graphs(epochs_list_LoHa, train_total_losses_LoHa, train_focal_losses_LoHa, train_bbox_losses_LoHa, train_giou_losses_LoHa, val_total_losses_LoHa)

# Load the saved model
model_with_LoHa = torch.load("loha_model.pth", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print(model_with_LoHa.peft_config)

# Check which weights have been updated
for name, param in model_with_LoHa.named_parameters():
    if param.requires_grad:
        print(f"{name} Trainable (Updated)")
    else:
        print(f"{name} Frozen (Original)")

print(model_with_LoHa)

# Mean value of LoHa layers after training
for name, param in model_with_LoHa.named_parameters():
    if "hada" in name:
        print(f"{name} mean after training: {param.data.mean().item():.6f}")

"""#### **Evaluate Model with LoHa**"""

model_with_LoHa_results = evaluate_model(model_with_LoHa, processor, test_dataset, yolos_categories)
print("Model with LoHa Performance (mAP):", model_with_LoHa_results)

model_with_LoHA_results_dict = {
    key: value.item() if isinstance(value, torch.Tensor) else value
    for key, value in model_with_LoHa_results.items()
    if key != "classes"
}
model_with_LoHa_results_df = pd.DataFrame([model_with_LoHA_results_dict])
print("Model with LoHa Performance (mAP):")
print(model_with_LoHa_results_df)
display(model_with_LoHa_results_df)

"""### **(d) LoKr- Low Rank Kronecker Product**

#### **LoKrConfig**
"""

# Define model for LoKr
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_for_LoKr = YolosForObjectDetection.from_pretrained(model_name).to(device)

lokr_config = LoKrConfig(
    r=8,
    alpha=16,
    rank_dropout=0.3,
    module_dropout=0.1,
    use_effective_conv2d=True,
    decompose_both=True,
    decompose_factor=8,
    target_modules=target_modules
)

# Apply LoKr to the model
model_with_LoKr = get_peft_model(model_for_LoKr, lokr_config)
model_with_LoKr.print_trainable_parameters()

# Inspect LoKr Layers
for name, param in model_with_LoKr.named_parameters():
    print(f"{name} - requires_grad: {param.requires_grad}")

for name, param in model_with_LoKr.named_parameters():
    if "lokr" in name:
        print(name, param.shape)

# Mean value of LoKr layers before training
for name, param in model_with_LoKr.named_parameters():
    if "lokr" in name:
        print(f"{name} mean after training: {param.data.mean().item():.6f}")

lokr_total_parameters = [p for p in model_with_LoKr.parameters()]
print(f"Number of LoKr parameter tensors: {len(lokr_total_parameters)}")

lokr_parameters = [p for p in model_with_LoKr.parameters() if p.requires_grad]
print(f"Number of LoKr trainable parameter tensors (requires_grad=True): {len(lokr_parameters)}")

"""#### **Train Model with LoKr**"""

# Train Setting
epochs = 10
optimizer = torch.optim.AdamW(lokr_parameters, lr=3e-5, betas=(0.9, 0.98), eps=1e-8, weight_decay=5e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold=0.001, verbose=True)
early_stopping = EarlyStopping(patience=3)

# Train the LoKr model
best_mAP_score_LoKr, epochs_list_LoKr, train_total_losses_LoKr, val_total_losses_LoKr, train_focal_losses_LoKr, train_bbox_losses_LoKr, train_giou_losses_LoKr, mAP_scores_LoKr = train_model(
    model_with_LoKr, processor, train_dataloader, val_dataloader, optimizer, scheduler, early_stopping, device, "lokr_model.pth", epochs=epochs, is_plateau_scheduler=True
)

plot_loss_graphs(epochs_list_LoKr, train_total_losses_LoKr, train_focal_losses_LoKr, train_bbox_losses_LoKr, train_giou_losses_LoKr, val_total_losses_LoKr)

# Load the saved model
model_with_LoKr = torch.load("lokr_model.pth", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print(model_with_LoKr.peft_config)

# Check which weights have been updated
for name, param in model_with_LoKr.named_parameters():
    if param.requires_grad:
        print(f"{name} Trainable (Updated)")
    else:
        print(f"{name} Frozen (Original)")

print(model_with_LoKr)

# Mean value of LoKr layers after training
for name, param in model_with_LoKr.named_parameters():
    if "lokr" in name:
        print(f"{name} mean after training: {param.data.mean().item():.6f}")

"""#### **Evaluate Model with LoKr**"""

model_with_LoKr_results = evaluate_model(model_with_LoKr, processor, test_dataset, yolos_categories)
print("Model with LoKr Performance (mAP):", model_with_LoKr_results)

model_with_LoKr_results_dict = {
    key: value.item() if isinstance(value, torch.Tensor) else value
    for key, value in model_with_LoKr_results.items()
    if key != "classes"
}
model_with_LoKr_results_df = pd.DataFrame([model_with_LoKr_results_dict])
print("Model with LoKr Performance (mAP):")
print(model_with_LoKr_results_df)
display(model_with_LoKr_results_df)

"""## **7. Models Comparison**

### Compare Models Results on Test dataset
"""

# Define the models and their respective DataFrames
models = ["Baseline", "LoRA", "AdaLoRA", "LoHa", "LoKr"]
dataframes = [baseline_results_df, model_with_LoRA_results_df, model_with_AdaLoRA_results_df,
              model_with_LoHa_results_df, model_with_LoKr_results_df]

# Combine results into a single table
results_list = []
for model, df in zip(models, dataframes):
    row = [model] + df.iloc[0].tolist()  # Extract the first row from each DataFrame
    results_list.append(row)

# Create a consolidated DataFrame
columns = ["Model"] + list(baseline_results_df.columns)
df_results = pd.DataFrame(results_list, columns=columns)

# Display the final comparison table
print("Model Comparison Results:")
display(df_results)

"""### Plots for Models Comparison"""

# Training and Validation Losses over Epochs
colors = {
    'LoRA': 'purple',
    'AdaLoRA': 'orange',
    'LoHa': 'cyan',
    'LoKr': 'brown'
}
epochs_lists = {
    'LoRA': epochs_list_LoRA,
    'AdaLoRA': epochs_list_AdaLoRA,
    'LoHa': epochs_list_LoHa,
    'LoKr': epochs_list_LoKr
}
train_losses_dict = {
    'LoRA': train_total_losses_LoRA,
    'AdaLoRA': train_total_losses_AdaLoRA,
    'LoHa': train_total_losses_LoHa,
    'LoKr': train_total_losses_LoKr
}
val_losses_dict = {
    'LoRA': val_total_losses_LoRA,
    'AdaLoRA': val_total_losses_AdaLoRA,
    'LoHa': val_total_losses_LoHa,
    'LoKr': val_total_losses_LoKr
}


# Train Losses
plt.figure(figsize=(10, 5))
for model, epochs_list in epochs_lists.items():
    train_losses = train_losses_dict.get(model, [])
    plt.plot(epochs_list, train_losses, label=f'{model} Training Loss', linestyle='-', marker='o', color=colors.get(model, 'black'))
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Validation Losses
plt.figure(figsize=(10, 5))
for model, epochs_list in epochs_lists.items():
    val_losses = val_losses_dict.get(model, [])
    plt.plot(epochs_list, val_losses, label=f'{model} Validation Loss', linestyle='--', marker='x', color=colors.get(model, 'black'))
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Validation Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

"""### Best Model - AdaLoRA"""

model_with_AdaLoRA = torch.load("adalora_model.pth", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
best_model_results = evaluate_model(model_with_AdaLoRA, processor, test_dataset, yolos_categories, add_class_metrices=True)
print("Best Model is AdaLoRA with Performance (mAP) of", best_model_results["map"])

# validate map_per_class values and then plot
if best_model_results["map_per_class"].numel() == 1 and best_model_results["map_per_class"].item() == -1:
    print("Warning: No valid data for per-class mAP.")
else:
    category_map = {coco_categories[i]: best_model_results["map_per_class"][i].item() for i in range(len(coco_categories))}
    selected_categories = random.sample(list(category_map), min(15, len(category_map)))
    color_map = plt.cm.get_cmap("tab20", 15)
    colors = [color_map(i) for i in range(15)]
    plt.figure(figsize=(10, 6))
    plt.bar(selected_categories, [category_map[cat] for cat in selected_categories], color=colors)
    plt.xticks(rotation=45)
    plt.ylabel('mAP per Category-AdaLoRA')
    plt.grid(False)
    plt.show()

